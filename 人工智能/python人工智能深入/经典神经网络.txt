LeNet-5：
LeNet 5在灰度图上进行训练，所以图层通道数目为1。
识别数字7,32 * 32 * 1的原始图像，5 * 5过滤器，步长为1，得到28 * 28 * 6的输出。
然后进行池化，在发表这篇文章时，人们更常用均值池化，虽然现在一般选择最大池化，f = 2，s = 2的滤波器进行均值池化，进行降维，得到14 * 14 * 6的体积结果。
接着进行另一层卷积。16个滤波器尺寸为5 * 5，得到10 * 10 * 16的结果。


AlexNet：
227 * 227 * 3原始图像，用96个11 * 11，步长为4的过滤器，得到55 * 55 * 96的输出，之后用3 * 3步长为2的滤波器进行最大池化，
体积降到27 * 27 * 96，然后用5 * 5进行卷积，相同填充，得到27 * 27 * 256，再次最大池化，得到13 * 13 * 256，然后相同的卷积，
相同填充，得到13 * 13 * 384然，然后再次3 * 3相同卷积，最后最大池化，得到6 * 6 * 256的输出。


VGG-16：
原始图像224 * 224 * 3，卷积池化卷积池化...最后得到7 * 7 * 512的结果输入全连接层，通过两层4096个神经元的全连接层，
然后是softmax函数输出1000类结果。

VGG-16结构简单，更注重卷积


ResNets（残差网络）：
太深的神经网络训练起来很难，因为有梯度消失和爆炸这类问题。
跳跃连接
残差网络是使用了残差结构的网络。这里有两层神经网络，a[l]代表第l层中的激活函数，然后到了第a[l+1]，两层之后是a[l+2]，
在这个计算步骤中，你先有一个a[l]，之后你做的第一件事就是将这个线性运算应用到它上面，即用a[l]计算z[l+1]，通过乘以一个加权矩阵，
并加上一个偏置向量，之后你通过非线性ReLu函数来得到a[l+1]，然后在下一层，再次应用这个线性步骤，最后再运行ReLu函数，得到a[l+2]。
换言之，从a[l]到a[l+2]的信息，它需要经过这些步骤，我们把这个步骤称为这组层的主路径。

在残差网络中，我们要做一点改变。
我们把a[l]，把它往前提，然后复制，提到神经网络非常靠后的位置，最后应用一些非线性处理，比如线性整流函数ReLu，我将这个称为快捷路径。



网中网，网络中的1x1卷积：
它本质上是一个完全连接的神经网络，逐一作用于这36个不同的位置上



Inception Network：
与其在卷积神经网络中选择一个你想使用的卷积核尺寸，乃至选择你是否需要一个卷积层还是一个池化层，让我们全部都做了吧。