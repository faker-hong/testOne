1.overfitting and underfitting
过拟合：在训练集上有好的目标效果，但在测试集上却不好。
        出现的可能为维度太多了，也就是特征太多了，一些不是决定结果的因素也考虑进去了

欠拟合：训练集与测试集上效果都不好
        出现的可能就是特征提取的不好，考虑的太少或者与结果不相关

2.Batch vs Stochastic Gradient Descent
批处理梯度下降(Batch Gradient Descent)：每一次迭代时使用所有样本来进行梯度的更新

优点：
  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
缺点：
  （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

随机梯度下降(Stochastic Gradient Descent):随机梯度下降是每次迭代使用一个样本来对参数进行更新

优点：
  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。
缺点：
  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。
  （3）不易于并行实现。

3.